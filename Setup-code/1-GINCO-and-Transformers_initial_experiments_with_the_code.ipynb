{"cells":[{"cell_type":"markdown","metadata":{"id":"lQtB43PuAD2H"},"source":["# Preparing the dataset"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":389,"status":"ok","timestamp":1640949108560,"user":{"displayName":"Taja Kuzman","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16199926972677634189"},"user_tz":-60},"id":"qL9iYqif_lSk"},"outputs":[],"source":["# Import the file\n","import json\n","\n","with open(\"/content/GINCO-1.0-suitable.json\") as f:\n","    dataset = json.load(f)\n","\n","#dataset[0]"]},{"cell_type":"markdown","metadata":{"id":"qn5ISqRR3hpq"},"source":[" ## Extract text from paragraphs into one string\n","\n"," We'll create two additional parameters for each text: \"full_text\" with text from all parameters, and \"dedup_text\" with text from the deduplicated paragraphs only (no near-duplicates)."]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1640949108561,"user":{"displayName":"Taja Kuzman","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16199926972677634189"},"user_tz":-60},"id":"T9jzjZSc3GX1"},"outputs":[],"source":["for instance in dataset:\n","    paragraphs = instance[\"paragraphs\"]\n","\n","    # Joining texts:\n","    instance_full_text = \" <p/> \".join([p[\"text\"] for p in paragraphs])\n","\n","    # Assigning texts to a new field:\n","    instance[\"full_text\"] = instance_full_text\n","\n","for instance in dataset:\n","    paragraphs = instance[\"paragraphs\"]\n","    # Removing duplicates:\n","    paragraphs = [p for p in paragraphs if not p[\"duplicate\"]]\n","\n","    # Joining texts:\n","    instance_dedup_text = \" <p/> \".join([p[\"text\"] for p in paragraphs])\n","\n","    # Assigning texts to a new field:\n","    instance[\"dedup_text\"] = instance_dedup_text\n","\n","#dataset[0]"]},{"cell_type":"markdown","metadata":{"id":"glWTirdA5MU6"},"source":["## Create the test-train-dev split"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1640949108562,"user":{"displayName":"Taja Kuzman","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16199926972677634189"},"user_tz":-60},"id":"HkxE4j5A5HQt","outputId":"ed3a4a61-8077-4f8b-a57d-1839e979de5b"},"outputs":[{"name":"stdout","output_type":"stream","text":["The train-dev-test splits consist of the following numbers of examples: 602 200 200\n"]}],"source":["train = [i for i in dataset if i[\"split\"] == \"train\"]\n","test = [i for i in dataset if i[\"split\"] == \"test\"]\n","dev = [i for i in dataset if i[\"split\"] == \"dev\"]\n","\n","print(\"The train-dev-test splits consist of the following numbers of examples:\", len(train), len(test), len(dev))"]},{"cell_type":"markdown","metadata":{"id":"xw1ugH8i6KuD"},"source":["## Transform the dataset in tabular form"]},{"cell_type":"markdown","metadata":{"id":"3o7RaayZBY7s"},"source":["As simpletransformers expects a pandas dataframe input, we now construct a DataFrame with columns text and labels.\n","\n","For labels we will use the primary_level_2 label and for the text, we'll use the deduplicated text."]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1640949108562,"user":{"displayName":"Taja Kuzman","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16199926972677634189"},"user_tz":-60},"id":"4APwEqqG5nF-"},"outputs":[],"source":["import pandas as pd\n","train_df = pd.DataFrame(data=train, columns=[\"dedup_text\", \"primary_level_2\"])\n","# Renaming columns to `text` and `labels`\n","train_df.columns = [\"text\", \"labels\"]\n","\n","# Let's look at the beginning of the train dataframe\n","\n","#train_df.head()"]},{"cell_type":"markdown","metadata":{"id":"bpdXwJ76CQng"},"source":["We will need to specify the exact number of labels, so we calculate it from our dataframe."]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1640949108563,"user":{"displayName":"Taja Kuzman","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16199926972677634189"},"user_tz":-60},"id":"1boL1xOFCSUc","outputId":"849f6aa4-b7e9-43b8-ffea-27fc49f91118"},"outputs":[{"data":{"text/plain":["21"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["LABELS = train_df.labels.unique().tolist()\n","NUM_LABELS = len(LABELS)\n","NUM_LABELS"]},{"cell_type":"markdown","metadata":{"id":"4Kx0NqxXE6vw"},"source":["Repeat the process with the test split:"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1640949108563,"user":{"displayName":"Taja Kuzman","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16199926972677634189"},"user_tz":-60},"id":"Hr30_WeJE_Bt"},"outputs":[],"source":["test_df = pd.DataFrame(data=test, columns=[\"dedup_text\", \"primary_level_2\"])\n","test_df.columns = [\"text\", \"labels\"]\n","#test_df.tail()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Save the dataframe to a csv:\n","train_df.to_csv(\"GINCO_dataframe_dedup_train.csv\", index=False)\n","test_df.to_csv(\"GINCO_dataframe_dedup_test.csv\", index=False)"]},{"cell_type":"markdown","metadata":{"id":"tmYURDJHCbRT"},"source":["# Training the baseline - SloBERTa"]},{"cell_type":"markdown","metadata":{"id":"9rLWz4ZPR3G6"},"source":["Resources:\n","- https://towardsdatascience.com/bert-text-classification-in-a-different-language-6af54930f9cb\n","- Peter's demo code: https://github.com/TajaKuzman/Transformers-GINCO-Experiments/blob/main/Peters-code/Peter-GINCO-demo.ipynb\n","- Peter's final code (for the LREC article): https://github.com/5roop/task5_webgenres/"]},{"cell_type":"markdown","metadata":{"id":"NAsW9gmZCt0k"},"source":["We will use the hyperparameters from the article *The GINCO Training Dataset for Web Genre Identification of Documents Out in the Wild* as the hyperparameter search revealed them to be the most suitable for the task. That is, the models will be trained for 30 epochs with the learning rate of 10^-5. The sequence length of 512 tokens will be used."]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12067,"status":"ok","timestamp":1640949120624,"user":{"displayName":"Taja Kuzman","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16199926972677634189"},"user_tz":-60},"id":"maK2UIUjEQyB","outputId":"4e5de5bb-7938-4fd5-b700-d4797f2b5475"},"outputs":[{"name":"stdout","output_type":"stream","text":["simpletransformers==0.63.3\n"]}],"source":["# install simpletransformers\n","!pip install -q simpletransformers\n","\n","# check installed version\n","!pip freeze | grep simpletransformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MZ-0GdV-Ccns"},"outputs":[],"source":["from simpletransformers.classification import ClassificationModel, ClassificationArgs\n","\n","\n","model_args = ClassificationArgs()\n","\n","model_args.num_train_epochs = 30\n","model_args.learning_rate = 1e-5\n","model_args.overwrite_output_dir = True\n","model_args.train_batch_size = 32\n","model_args.no_cache = True\n","model_args.no_save = True\n","model_args.fp16 = False\n","model_args.save_steps = -1\n","model_args.max_seq_length = 512\n","model_args.labels_list = LABELS\n","\n","\n","model = ClassificationModel(\"camembert\", \"EMBEDDIA/sloberta\",\n","                            num_labels = NUM_LABELS,\n","                            use_cuda = True,\n","                            args = model_args,\n","                            )\n","model.train_model(train_df)"]},{"cell_type":"markdown","metadata":{"id":"1fqqBKSB2Vea"},"source":["There are some issues with the model. I've checked whether Google Colab otherwise works with simple transformers (using an example from a BERT tutorial), and it does."]},{"cell_type":"markdown","metadata":{"id":"HATgrpdgC4q1"},"source":["Working with this setting worked, so we must find out which of the settings in the initial code produced an error."]},{"cell_type":"markdown","metadata":{"id":"fH9Th9keFPIl"},"source":["First, I've muted additional parameters from the BERT tutorial. The code worked, so I deleted them."]},{"cell_type":"markdown","metadata":{"id":"_hbu7RVaDEW5"},"source":["1. Overwrite_output_dir - okay\n","2. Num_train_epochs - okay\n","3. labels_list - okay\n","4. learning_rate - okay\n","5. train_batch_size - okay\n","6. no_cache - okay\n","7. no_save - okay\n","8. save_steps - okay\n","\n","--> the problem is in the parameter **\"max_seq_length\": 512** When trying the sliding_window method, which can be used for texts, longer than 512 tokens, it was written that the max seq length is 512 (Token indices sequence length is longer than the specified maximum sequence length for this model (622 > 512).), so maybe there is no need to use this parameter.\n","\n","Peter's code (from demo) does not work as well."]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"GINCO-and-Transformers_initial_experiments_with_the_code.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
