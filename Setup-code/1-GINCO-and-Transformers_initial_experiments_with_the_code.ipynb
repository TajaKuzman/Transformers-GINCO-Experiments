{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Preparing the dataset","metadata":{"id":"lQtB43PuAD2H"}},{"cell_type":"code","source":"# Import the file\nimport json\n\nwith open(\"/kaggle/input/genre-identification-corpus-ginco-10/GINCO-1.0-suitable.json\") as f:\n    dataset = json.load(f)\n\ndataset[0]","metadata":{"executionInfo":{"elapsed":389,"status":"ok","timestamp":1640949108560,"user":{"displayName":"Taja Kuzman","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16199926972677634189"},"user_tz":-60},"id":"qL9iYqif_lSk","execution":{"iopub.status.busy":"2022-01-03T13:10:39.695203Z","iopub.execute_input":"2022-01-03T13:10:39.695476Z","iopub.status.idle":"2022-01-03T13:10:39.875903Z","shell.execute_reply.started":"2022-01-03T13:10:39.695449Z","shell.execute_reply":"2022-01-03T13:10:39.875075Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":" ## Extract text from paragraphs into one string\n\n We'll create two additional parameters for each text: \"full_text\" with text from all parameters, and \"dedup_text\" with text from the deduplicated paragraphs only (no near-duplicates).","metadata":{"id":"qn5ISqRR3hpq"}},{"cell_type":"code","source":"for instance in dataset:\n    paragraphs = instance[\"paragraphs\"]\n\n    # Joining texts:\n    instance_full_text = \" <p/> \".join([p[\"text\"] for p in paragraphs])\n\n    # Assigning texts to a new field:\n    instance[\"full_text\"] = instance_full_text\n\nfor instance in dataset:\n    paragraphs = instance[\"paragraphs\"]\n    # Removing duplicates:\n    paragraphs = [p for p in paragraphs if not p[\"duplicate\"]]\n\n    # Joining texts:\n    instance_dedup_text = \" <p/> \".join([p[\"text\"] for p in paragraphs])\n\n    # Assigning texts to a new field:\n    instance[\"dedup_text\"] = instance_dedup_text\n\ndataset[0]","metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1640949108561,"user":{"displayName":"Taja Kuzman","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16199926972677634189"},"user_tz":-60},"id":"T9jzjZSc3GX1","execution":{"iopub.status.busy":"2022-01-03T13:11:21.101832Z","iopub.execute_input":"2022-01-03T13:11:21.102689Z","iopub.status.idle":"2022-01-03T13:11:21.124685Z","shell.execute_reply.started":"2022-01-03T13:11:21.102639Z","shell.execute_reply":"2022-01-03T13:11:21.123914Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Create the test-train-dev split","metadata":{"id":"glWTirdA5MU6"}},{"cell_type":"code","source":"train = [i for i in dataset if i[\"split\"] == \"train\"]\ntest = [i for i in dataset if i[\"split\"] == \"test\"]\ndev = [i for i in dataset if i[\"split\"] == \"dev\"]\n\nprint(\"The train-dev-test splits consist of the following numbers of examples:\", len(train), len(test), len(dev))","metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1640949108562,"user":{"displayName":"Taja Kuzman","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16199926972677634189"},"user_tz":-60},"id":"HkxE4j5A5HQt","outputId":"ed3a4a61-8077-4f8b-a57d-1839e979de5b","execution":{"iopub.status.busy":"2022-01-03T13:11:41.465052Z","iopub.execute_input":"2022-01-03T13:11:41.465329Z","iopub.status.idle":"2022-01-03T13:11:41.473567Z","shell.execute_reply.started":"2022-01-03T13:11:41.465297Z","shell.execute_reply":"2022-01-03T13:11:41.472580Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Transform the dataset in tabular form","metadata":{"id":"xw1ugH8i6KuD"}},{"cell_type":"markdown","source":"As simpletransformers expects a pandas dataframe input, we now construct a DataFrame with columns text and labels.\n\nFor labels we will use the primary_level_2 label and for the text, we'll use the deduplicated text.","metadata":{"id":"3o7RaayZBY7s"}},{"cell_type":"code","source":"import pandas as pd\ntrain_df = pd.DataFrame(data=train, columns=[\"dedup_text\", \"primary_level_2\"])\n# Renaming columns to `text` and `labels`\ntrain_df.columns = [\"text\", \"labels\"]\n\n# Let's look at the beginning of the train dataframe\n\ntrain_df.head()","metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1640949108562,"user":{"displayName":"Taja Kuzman","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16199926972677634189"},"user_tz":-60},"id":"4APwEqqG5nF-","execution":{"iopub.status.busy":"2022-01-03T13:12:02.493452Z","iopub.execute_input":"2022-01-03T13:12:02.493860Z","iopub.status.idle":"2022-01-03T13:12:02.513209Z","shell.execute_reply.started":"2022-01-03T13:12:02.493822Z","shell.execute_reply":"2022-01-03T13:12:02.512632Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"We will need to specify the exact number of labels, so we calculate it from our dataframe.","metadata":{"id":"bpdXwJ76CQng"}},{"cell_type":"code","source":"LABELS = train_df.labels.unique().tolist()\nNUM_LABELS = len(LABELS)\nNUM_LABELS","metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1640949108563,"user":{"displayName":"Taja Kuzman","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16199926972677634189"},"user_tz":-60},"id":"1boL1xOFCSUc","outputId":"849f6aa4-b7e9-43b8-ffea-27fc49f91118","execution":{"iopub.status.busy":"2022-01-03T13:12:09.001298Z","iopub.execute_input":"2022-01-03T13:12:09.001907Z","iopub.status.idle":"2022-01-03T13:12:09.015478Z","shell.execute_reply.started":"2022-01-03T13:12:09.001857Z","shell.execute_reply":"2022-01-03T13:12:09.014643Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Repeat the process with the test and dev split:","metadata":{"id":"4Kx0NqxXE6vw"}},{"cell_type":"code","source":"test_df = pd.DataFrame(data=test, columns=[\"dedup_text\", \"primary_level_2\"])\ntest_df.columns = [\"text\", \"labels\"]\ntest_df.tail()","metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1640949108563,"user":{"displayName":"Taja Kuzman","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16199926972677634189"},"user_tz":-60},"id":"Hr30_WeJE_Bt","execution":{"iopub.status.busy":"2022-01-03T13:12:28.974871Z","iopub.execute_input":"2022-01-03T13:12:28.975500Z","iopub.status.idle":"2022-01-03T13:12:28.987847Z","shell.execute_reply.started":"2022-01-03T13:12:28.975439Z","shell.execute_reply":"2022-01-03T13:12:28.987006Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"dev_df = pd.DataFrame(data=dev, columns=[\"dedup_text\", \"primary_level_2\"])\ndev_df.columns = [\"text\", \"labels\"]\ndev_df.tail()","metadata":{"execution":{"iopub.status.busy":"2022-01-03T13:12:55.843961Z","iopub.execute_input":"2022-01-03T13:12:55.844266Z","iopub.status.idle":"2022-01-03T13:12:55.855933Z","shell.execute_reply.started":"2022-01-03T13:12:55.844233Z","shell.execute_reply":"2022-01-03T13:12:55.854982Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Save the dataframe to a csv:\ntrain_df.to_csv(\"GINCO_dataframe_dedup_train.csv\", index=False)\ntest_df.to_csv(\"GINCO_dataframe_dedup_test.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-03T13:13:11.313935Z","iopub.execute_input":"2022-01-03T13:13:11.314214Z","iopub.status.idle":"2022-01-03T13:13:11.392187Z","shell.execute_reply.started":"2022-01-03T13:13:11.314184Z","shell.execute_reply":"2022-01-03T13:13:11.391379Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"Merge train to dev to get bigger train data:","metadata":{}},{"cell_type":"code","source":"train_dev_df = pd.concat([train_df, dev_df], ignore_index=True)\ntrain_dev_df.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-03T13:14:55.578418Z","iopub.execute_input":"2022-01-03T13:14:55.578716Z","iopub.status.idle":"2022-01-03T13:14:55.585917Z","shell.execute_reply.started":"2022-01-03T13:14:55.578683Z","shell.execute_reply":"2022-01-03T13:14:55.585393Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Save the dataframe to a csv:\ntrain_dev_df.to_csv(\"GINCO_dataframe_dedup_train_dev.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-03T13:15:21.955321Z","iopub.execute_input":"2022-01-03T13:15:21.956048Z","iopub.status.idle":"2022-01-03T13:15:22.032779Z","shell.execute_reply.started":"2022-01-03T13:15:21.956005Z","shell.execute_reply":"2022-01-03T13:15:22.031958Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# Training the baseline - SloBERTa","metadata":{"id":"tmYURDJHCbRT"}},{"cell_type":"markdown","source":"Resources:\n- https://towardsdatascience.com/bert-text-classification-in-a-different-language-6af54930f9cb\n- Peter's demo code: https://github.com/TajaKuzman/Transformers-GINCO-Experiments/blob/main/Peters-code/Peter-GINCO-demo.ipynb\n- Peter's final code (for the LREC article): https://github.com/5roop/task5_webgenres/","metadata":{"id":"9rLWz4ZPR3G6"}},{"cell_type":"markdown","source":"We will use the hyperparameters from the article *The GINCO Training Dataset for Web Genre Identification of Documents Out in the Wild* as the hyperparameter search revealed them to be the most suitable for the task. That is, the models will be trained for 30 epochs with the learning rate of 10^-5. The sequence length of 512 tokens will be used.","metadata":{"id":"NAsW9gmZCt0k"}},{"cell_type":"code","source":"# install simpletransformers\n!pip install -q simpletransformers\n\n# check installed version\n!pip freeze | grep simpletransformers","metadata":{"executionInfo":{"elapsed":12067,"status":"ok","timestamp":1640949120624,"user":{"displayName":"Taja Kuzman","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16199926972677634189"},"user_tz":-60},"id":"maK2UIUjEQyB","outputId":"4e5de5bb-7938-4fd5-b700-d4797f2b5475"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from simpletransformers.classification import ClassificationModel, ClassificationArgs\n\n\nmodel_args = ClassificationArgs()\n\nmodel_args.num_train_epochs = 30\nmodel_args.learning_rate = 1e-5\nmodel_args.overwrite_output_dir = True\nmodel_args.train_batch_size = 32\nmodel_args.no_cache = True\nmodel_args.no_save = True\nmodel_args.fp16 = False\nmodel_args.save_steps = -1\nmodel_args.max_seq_length = 512\nmodel_args.labels_list = LABELS\n\n\nmodel = ClassificationModel(\"camembert\", \"EMBEDDIA/sloberta\",\n                            num_labels = NUM_LABELS,\n                            use_cuda = True,\n                            args = model_args,\n                            )\nmodel.train_model(train_df)","metadata":{"id":"MZ-0GdV-Ccns"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are some issues with the model. I've checked whether Google Colab otherwise works with simple transformers (using an example from a BERT tutorial), and it does.","metadata":{"id":"1fqqBKSB2Vea"}},{"cell_type":"markdown","source":"Working with this setting worked, so we must find out which of the settings in the initial code produced an error.","metadata":{"id":"HATgrpdgC4q1"}},{"cell_type":"markdown","source":"First, I've muted additional parameters from the BERT tutorial. The code worked, so I deleted them.","metadata":{"id":"fH9Th9keFPIl"}},{"cell_type":"markdown","source":"1. Overwrite_output_dir - okay\n2. Num_train_epochs - okay\n3. labels_list - okay\n4. learning_rate - okay\n5. train_batch_size - okay\n6. no_cache - okay\n7. no_save - okay\n8. save_steps - okay\n\n--> the problem is in the parameter **\"max_seq_length\": 512** When trying the sliding_window method, which can be used for texts, longer than 512 tokens, it was written that the max seq length is 512 (Token indices sequence length is longer than the specified maximum sequence length for this model (622 > 512).), so maybe there is no need to use this parameter.\n\nPeter's code (from demo) does not work as well.","metadata":{"id":"_hbu7RVaDEW5"}}]}