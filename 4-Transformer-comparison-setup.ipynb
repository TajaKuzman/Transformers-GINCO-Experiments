{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll prepare the code for all transformers, so that we'll see whether the hyperparameters (especially the max_seq_length parameter which causes errors) work on all of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SloBERTa\n",
    "Slovene model\n",
    "https://huggingface.co/EMBEDDIA/sloberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sloberta_model(train_df, labels=None, max_seq_length_no=300):\n",
    "    \"\"\"Trains a simpletransformer model and returns it.\n",
    "\n",
    "    Args:\n",
    "        train_df (pandas.DataFrame): A DataFrame with columns [\"text\", \"labels\"].\n",
    "        labels (list(str), optional): If not None, use these labels to use string labels instead of numeric labels. \n",
    "            Defaults to None.\n",
    "        max_seq_length_no (int, optional): Defaults to 300.\n",
    "\n",
    "    Returns:\n",
    "        simpletransformers.ClassificationModel: a trained model\n",
    "    \"\"\"\n",
    "    from simpletransformers.classification import ClassificationModel\n",
    "\n",
    "    # define hyperparameter\n",
    "     model_args ={\"overwrite_output_dir\": True,\n",
    "             \"num_train_epochs\": 90,\n",
    "             \"labels_list\": LABELS,\n",
    "             \"learning_rate\": 1e-5,\n",
    "             \"train_batch_size\": 32,\n",
    "             \"no_cache\": True,\n",
    "             \"no_save\": True,\n",
    "             \"max_seq_length\": max_seq_length_no,\n",
    "             \"save_steps\": -1,\n",
    "             }\n",
    "\n",
    "\n",
    "    model = ClassificationModel(\n",
    "        \"camembert\", \"EMBEDDIA/sloberta\",\n",
    "        num_labels=21,\n",
    "        use_cuda=True,\n",
    "        args=model_args\n",
    "    )\n",
    "    model.train_model(train_df)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CroSloEngual BERT\n",
    "Slovene-Croatian-English model\n",
    "https://huggingface.co/EMBEDDIA/crosloengual-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crosloengualbert_model(train_df, labels=None, max_seq_length_no=300):\n",
    "    \"\"\"Trains a simpletransformer model and returns it.\n",
    "\n",
    "    Args:\n",
    "        train_df (pandas.DataFrame): A DataFrame with columns [\"text\", \"labels\"].\n",
    "        labels (list(str), optional): If not None, use these labels to use string labels instead of numeric labels. \n",
    "            Defaults to None.\n",
    "        max_seq_length_no (int, optional): Defaults to 300.\n",
    "\n",
    "    Returns:\n",
    "        simpletransformers.ClassificationModel: a trained model\n",
    "    \"\"\"\n",
    "    from simpletransformers.classification import ClassificationModel\n",
    "\n",
    "    # define hyperparameter\n",
    "     model_args ={\"overwrite_output_dir\": True,\n",
    "             \"num_train_epochs\": 90,\n",
    "             \"labels_list\": LABELS,\n",
    "             \"learning_rate\": 1e-5,\n",
    "             \"train_batch_size\": 32,\n",
    "             \"no_cache\": True,\n",
    "             \"no_save\": True,\n",
    "             \"max_seq_length\": max_seq_length_no,\n",
    "             \"save_steps\": -1,\n",
    "             }\n",
    "\n",
    "\n",
    "    model = ClassificationModel(\n",
    "        \"bert\", \"EMBEDDIA/crosloengual-bert\",\n",
    "        num_labels=21,\n",
    "        use_cuda=True,\n",
    "        args=model_args\n",
    "    )\n",
    "    model.train_model(train_df)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base-sized XLM-RoBERTa\n",
    "\n",
    "Multilingual model\n",
    "https://huggingface.co/xlm-roberta-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roberta_base_model(train_df, labels=None, max_seq_length_no=300):\n",
    "    \"\"\"Trains a simpletransformer model and returns it.\n",
    "\n",
    "    Args:\n",
    "        train_df (pandas.DataFrame): A DataFrame with columns [\"text\", \"labels\"].\n",
    "        labels (list(str), optional): If not None, use these labels to use string labels instead of numeric labels. \n",
    "            Defaults to None.\n",
    "        max_seq_length_no (int, optional): Defaults to 300.\n",
    "\n",
    "    Returns:\n",
    "        simpletransformers.ClassificationModel: a trained model\n",
    "    \"\"\"\n",
    "    from simpletransformers.classification import ClassificationModel\n",
    "\n",
    "    # define hyperparameter\n",
    "     model_args ={\"overwrite_output_dir\": True,\n",
    "             \"num_train_epochs\": 90,\n",
    "             \"labels_list\": LABELS,\n",
    "             \"learning_rate\": 1e-5,\n",
    "             \"train_batch_size\": 32,\n",
    "             \"no_cache\": True,\n",
    "             \"no_save\": True,\n",
    "             \"max_seq_length\": max_seq_length_no,\n",
    "             \"save_steps\": -1,\n",
    "             }\n",
    "\n",
    "\n",
    "    model = ClassificationModel(\n",
    "        \"xlm-roberta\", \"xlm-roberta-base\",\n",
    "        num_labels=21,\n",
    "        use_cuda=True,\n",
    "        args=model_args\n",
    "    )\n",
    "    model.train_model(train_df)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large-sized XML-RoBERTa\n",
    "Multilingual model https://huggingface.co/xlm-roberta-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roberta_large_model(train_df, labels=None, max_seq_length_no=300):\n",
    "    \"\"\"Trains a simpletransformer model and returns it.\n",
    "\n",
    "    Args:\n",
    "        train_df (pandas.DataFrame): A DataFrame with columns [\"text\", \"labels\"].\n",
    "        labels (list(str), optional): If not None, use these labels to use string labels instead of numeric labels. \n",
    "            Defaults to None.\n",
    "        max_seq_length_no (int, optional): Defaults to 300.\n",
    "\n",
    "    Returns:\n",
    "        simpletransformers.ClassificationModel: a trained model\n",
    "    \"\"\"\n",
    "    from simpletransformers.classification import ClassificationModel\n",
    "\n",
    "    # define hyperparameter\n",
    "     model_args ={\"overwrite_output_dir\": True,\n",
    "             \"num_train_epochs\": 90,\n",
    "             \"labels_list\": LABELS,\n",
    "             \"learning_rate\": 1e-5,\n",
    "             \"train_batch_size\": 32,\n",
    "             \"no_cache\": True,\n",
    "             \"no_save\": True,\n",
    "             \"max_seq_length\": max_seq_length_no,\n",
    "             \"save_steps\": -1,\n",
    "             }\n",
    "\n",
    "\n",
    "    model = ClassificationModel(\n",
    "        \"xlm-roberta\", \"xlm-roberta-large\",\n",
    "        num_labels=21,\n",
    "        use_cuda=True,\n",
    "        args=model_args\n",
    "    )\n",
    "    model.train_model(train_df)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeBERTaV3\n",
    "Multilingual model https://huggingface.co/microsoft/mdeberta-v3-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debertav3_model(train_df, labels=None, max_seq_length_no=300):\n",
    "    \"\"\"Trains a simpletransformer model and returns it.\n",
    "\n",
    "    Args:\n",
    "        train_df (pandas.DataFrame): A DataFrame with columns [\"text\", \"labels\"].\n",
    "        labels (list(str), optional): If not None, use these labels to use string labels instead of numeric labels. \n",
    "            Defaults to None.\n",
    "        max_seq_length_no (int, optional): Defaults to 300.\n",
    "\n",
    "    Returns:\n",
    "        simpletransformers.ClassificationModel: a trained model\n",
    "    \"\"\"\n",
    "    from simpletransformers.classification import ClassificationModel\n",
    "\n",
    "    # define hyperparameter\n",
    "     model_args ={\"overwrite_output_dir\": True,\n",
    "             \"num_train_epochs\": 90,\n",
    "             \"labels_list\": LABELS,\n",
    "             \"learning_rate\": 1e-5,\n",
    "             \"train_batch_size\": 32,\n",
    "             \"no_cache\": True,\n",
    "             \"no_save\": True,\n",
    "             \"max_seq_length\": max_seq_length_no,\n",
    "             \"save_steps\": -1,\n",
    "             }\n",
    "\n",
    "\n",
    "    model = ClassificationModel(\n",
    "        \"deberta-v2\", \"microsoft/mdeberta-v3-base\",\n",
    "        num_labels=21,\n",
    "        use_cuda=True,\n",
    "        args=model_args\n",
    "    )\n",
    "    model.train_model(train_df)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERTić\n",
    "Model for related South Slavic languages https://huggingface.co/classla/bcms-bertic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bertic_model(train_df, labels=None, max_seq_length_no=300):\n",
    "    \"\"\"Trains a simpletransformer model and returns it.\n",
    "\n",
    "    Args:\n",
    "        train_df (pandas.DataFrame): A DataFrame with columns [\"text\", \"labels\"].\n",
    "        labels (list(str), optional): If not None, use these labels to use string labels instead of numeric labels. \n",
    "            Defaults to None.\n",
    "        max_seq_length_no (int, optional): Defaults to 300.\n",
    "\n",
    "    Returns:\n",
    "        simpletransformers.ClassificationModel: a trained model\n",
    "    \"\"\"\n",
    "    from simpletransformers.classification import ClassificationModel\n",
    "\n",
    "    # define hyperparameter\n",
    "     model_args ={\"overwrite_output_dir\": True,\n",
    "             \"num_train_epochs\": 90,\n",
    "             \"labels_list\": LABELS,\n",
    "             \"learning_rate\": 1e-5,\n",
    "             \"train_batch_size\": 32,\n",
    "             \"no_cache\": True,\n",
    "             \"no_save\": True,\n",
    "             \"max_seq_length\": max_seq_length_no,\n",
    "             \"save_steps\": -1,\n",
    "             }\n",
    "\n",
    "\n",
    "    model = ClassificationModel(\n",
    "        \"electra\", \"classla/bcms-bertic\",\n",
    "        num_labels=21,\n",
    "        use_cuda=True,\n",
    "        args=model_args\n",
    "    )\n",
    "    model.train_model(train_df)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT base model (cased)\n",
    "Monolingual English model https://huggingface.co/bert-base-cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bertbase_model(train_df, labels=None, max_seq_length_no=300):\n",
    "    \"\"\"Trains a simpletransformer model and returns it.\n",
    "\n",
    "    Args:\n",
    "        train_df (pandas.DataFrame): A DataFrame with columns [\"text\", \"labels\"].\n",
    "        labels (list(str), optional): If not None, use these labels to use string labels instead of numeric labels. \n",
    "            Defaults to None.\n",
    "        max_seq_length_no (int, optional): Defaults to 300.\n",
    "\n",
    "    Returns:\n",
    "        simpletransformers.ClassificationModel: a trained model\n",
    "    \"\"\"\n",
    "    from simpletransformers.classification import ClassificationModel\n",
    "\n",
    "    # define hyperparameter\n",
    "     model_args ={\"overwrite_output_dir\": True,\n",
    "             \"num_train_epochs\": 90,\n",
    "             \"labels_list\": LABELS,\n",
    "             \"learning_rate\": 1e-5,\n",
    "             \"train_batch_size\": 32,\n",
    "             \"no_cache\": True,\n",
    "             \"no_save\": True,\n",
    "             \"max_seq_length\": max_seq_length_no,\n",
    "             \"save_steps\": -1,\n",
    "             }\n",
    "\n",
    "\n",
    "    model = ClassificationModel(\n",
    "        \"bert\", \"bert-base-cased\",\n",
    "        num_labels=21,\n",
    "        use_cuda=True,\n",
    "        args=model_args\n",
    "    )\n",
    "    model.train_model(train_df)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Import the data, prepared for the experiments\n",
    "train_df = pd.read_csv(\"/kaggle/input/gincodataframededuptraindevtest/GINCO_dataframe_dedup_train_dev.csv\")\n",
    "test_df = pd.read_csv(\"/kaggle//input/gincodataframededuptraindevtest/GINCO_dataframe_dedup_test.csv\")\n",
    "\n",
    "print(\"Train shape: {}, Test shape: {}.\".format(train_df.shape, test_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of labels\n",
    "LABELS = train_df.labels.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the instances with no text\n",
    "train_df = train_df.dropna()\n",
    "test_df = test_df.dropna()\n",
    "print(\"Train shape: {}, Test shape: {}.\".format(train_df.shape, test_df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all necessary libraries and install everything you need for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install pytorch\n",
    "!conda install --yes pytorch>=1.6 cudatoolkit=11.0 -c pytorch\n",
    "\n",
    "# install simpletransformers\n",
    "!pip install -q transformers\n",
    "!pip install --upgrade transformers\n",
    "!pip install -q simpletransformers\n",
    "\n",
    "# check installed version\n",
    "!pip freeze | grep simpletransformers\n",
    "\n",
    "# install stable torch\n",
    "!pip uninstall -q torch -y\n",
    "!pip install -q torch==1.6.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n",
    "# install the libraries necessary for prediction and result analysis\n",
    "from sklearn.metrics import f1_score, ConfusionMatrixDisplay, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SloBERTa\n",
    "sloberta_model(train_df)\n",
    "\n",
    "# CroSloEngual BERT\n",
    "#crosloengualbert_model(train_df)\n",
    "\n",
    "# Base-sized XML-Roberta\n",
    "#roberta_base_model(train_df)\n",
    "\n",
    "# Large-sized XML-Roberta\n",
    "#roberta_large_model(train_df)\n",
    "\n",
    "# DeBERTav3\n",
    "#debertav3_model(train_df)\n",
    "\n",
    "# BERTić\n",
    "#bertic_model(train_df)\n",
    "\n",
    "# English base-sized BERT\n",
    "#bertbase_model(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e75iABBeiKZg"
   },
   "source": [
    "Let's try if the model works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-03T14:27:48.085221Z",
     "iopub.status.busy": "2022-01-03T14:27:48.084742Z",
     "iopub.status.idle": "2022-01-03T14:27:48.425076Z",
     "shell.execute_reply": "2022-01-03T14:27:48.424346Z",
     "shell.execute_reply.started": "2022-01-03T14:27:48.085184Z"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "aborted",
     "timestamp": 1641043613768,
     "user": {
      "displayName": "Taja Kuzman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16199926972677634189"
     },
     "user_tz": -60
    },
    "id": "LOVsS_T2g8M6",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "Instance_predictions, raw_outputs = model.predict(['Danes poročamo o dogodku, ki se je zgodil 1. 1. 2020. Oseba je dejala:\"To je res nenormalen dogodek\"'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-03T14:27:48.427166Z",
     "iopub.status.busy": "2022-01-03T14:27:48.426737Z",
     "iopub.status.idle": "2022-01-03T14:27:48.432870Z",
     "shell.execute_reply": "2022-01-03T14:27:48.432209Z",
     "shell.execute_reply.started": "2022-01-03T14:27:48.427123Z"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "aborted",
     "timestamp": 1641043613768,
     "user": {
      "displayName": "Taja Kuzman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16199926972677634189"
     },
     "user_tz": -60
    },
    "id": "2W1JXnu_iIiS",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "Instance_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-03T14:27:48.434691Z",
     "iopub.status.busy": "2022-01-03T14:27:48.434243Z",
     "iopub.status.idle": "2022-01-03T14:27:51.563105Z",
     "shell.execute_reply": "2022-01-03T14:27:51.562235Z",
     "shell.execute_reply.started": "2022-01-03T14:27:48.434653Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Get the true labels from the dataframe\n",
    "y_true = test_df.labels\n",
    "\n",
    "# Calculate the model's predictions\n",
    "y_pred = model.predict(test_df.text.tolist())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the macro and micro F1 scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-03T14:27:51.565209Z",
     "iopub.status.busy": "2022-01-03T14:27:51.564932Z",
     "iopub.status.idle": "2022-01-03T14:27:51.582860Z",
     "shell.execute_reply": "2022-01-03T14:27:51.582151Z",
     "shell.execute_reply.started": "2022-01-03T14:27:51.565165Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "macro = f1_score(y_true, y_pred, labels=LABELS, average=\"macro\")\n",
    "micro = f1_score(y_true, y_pred, labels=LABELS,  average=\"micro\")\n",
    "print(f\"Macro f1: {macro:0.3}\\nMicro f1: {micro:0.3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produce the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-03T14:38:27.091369Z",
     "iopub.status.busy": "2022-01-03T14:38:27.091102Z",
     "iopub.status.idle": "2022-01-03T14:38:27.100057Z",
     "shell.execute_reply": "2022-01-03T14:38:27.099371Z",
     "shell.execute_reply.started": "2022-01-03T14:38:27.091340Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_cm(y_true, y_pred, labels, title=None):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels, )\n",
    "    # print(cm)\n",
    "    plt.figure(figsize=(9, 9))\n",
    "    plt.imshow(cm, cmap=\"Oranges\")\n",
    "    for (i, j), z in np.ndenumerate(cm):\n",
    "        plt.text(j, i, '{:d}'.format(z), ha='center', va='center')\n",
    "    classNames = labels\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    tick_marks = np.arange(len(classNames))\n",
    "    plt.xticks(tick_marks, classNames, rotation=90)\n",
    "    plt.yticks(tick_marks, classNames)\n",
    "    microF1 = f1_score(y_true, y_pred, labels=labels, average =\"micro\")\n",
    "    macroF1 = f1_score(y_true, y_pred, labels=labels, average =\"macro\")\n",
    "\n",
    "    print(f\"Micro F1: {microF1:0.3}\",f\"Macro F1: {macroF1:0.3}\")\n",
    "\n",
    "    metrics = f\"{microF1:0.4}, {macroF1:0.4}\"\n",
    "    if title:\n",
    "        plt.title(title +\";\\n\" + metrics)\n",
    "    else:\n",
    "        plt.title(metrics)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return microF1, macroF1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-03T14:38:42.309968Z",
     "iopub.status.busy": "2022-01-03T14:38:42.309673Z",
     "iopub.status.idle": "2022-01-03T14:38:43.935420Z",
     "shell.execute_reply": "2022-01-03T14:38:43.934694Z",
     "shell.execute_reply.started": "2022-01-03T14:38:42.309937Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plot_cm(y_true, y_pred, LABELS, title=\"SloBERTa-Initial_Setup_test_dev\")\n",
    "#plt.savefig(\"SloBERTa-Initial-Setup_test_dev.png\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
