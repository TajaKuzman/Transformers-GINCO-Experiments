{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"In this notebook, we'll prepare the code for all transformers, so that we'll see whether the hyperparameters (especially the max_seq_length parameter which causes errors) work on all of them.","metadata":{}},{"cell_type":"markdown","source":"Import all necessary libraries and install everything you need for training:","metadata":{}},{"cell_type":"code","source":"# install pytorch\n!conda install --yes pytorch>=1.6 cudatoolkit=11.0 -c pytorch","metadata":{"execution":{"iopub.status.busy":"2022-01-03T20:15:39.296632Z","iopub.execute_input":"2022-01-03T20:15:39.297034Z","iopub.status.idle":"2022-01-03T20:16:18.161423Z","shell.execute_reply.started":"2022-01-03T20:15:39.296955Z","shell.execute_reply":"2022-01-03T20:16:18.160456Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# install simpletransformers\n!pip install -q transformers\n!pip install --upgrade transformers\n!pip install -q simpletransformers\n\n# check installed version\n!pip freeze | grep simpletransformers","metadata":{"execution":{"iopub.status.busy":"2022-01-03T20:16:18.166994Z","iopub.execute_input":"2022-01-03T20:16:18.167468Z","iopub.status.idle":"2022-01-03T20:16:44.016148Z","shell.execute_reply.started":"2022-01-03T20:16:18.167427Z","shell.execute_reply":"2022-01-03T20:16:44.015296Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# install stable torch\n!pip uninstall -q torch -y\n!pip install -q torch==1.6.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html","metadata":{"execution":{"iopub.status.busy":"2022-01-03T20:16:44.019647Z","iopub.execute_input":"2022-01-03T20:16:44.019869Z","iopub.status.idle":"2022-01-03T20:17:25.393857Z","shell.execute_reply.started":"2022-01-03T20:16:44.019840Z","shell.execute_reply":"2022-01-03T20:17:25.392940Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# install the libraries necessary for data wrangling, prediction and result analysis\nfrom sklearn.metrics import f1_score, ConfusionMatrixDisplay, confusion_matrix, accuracy_score\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2022-01-03T20:17:25.396850Z","iopub.execute_input":"2022-01-03T20:17:25.397433Z","iopub.status.idle":"2022-01-03T20:17:25.786821Z","shell.execute_reply.started":"2022-01-03T20:17:25.397380Z","shell.execute_reply":"2022-01-03T20:17:25.786073Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### Import the data","metadata":{}},{"cell_type":"code","source":"# Import the data, prepared for the experiments\ntrain_df = pd.read_csv(\"/kaggle/input/gincodataframededuptraindevtest/GINCO_dataframe_dedup_train_dev.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/gincodataframededuptraindevtest/GINCO_dataframe_dedup_test.csv\")\n\nprint(\"Train shape: {}, Test shape: {}.\".format(train_df.shape, test_df.shape))","metadata":{"execution":{"iopub.status.busy":"2022-01-03T20:17:25.788182Z","iopub.execute_input":"2022-01-03T20:17:25.788492Z","iopub.status.idle":"2022-01-03T20:17:25.834217Z","shell.execute_reply.started":"2022-01-03T20:17:25.788452Z","shell.execute_reply":"2022-01-03T20:17:25.833504Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Create a list of labels\nLABELS = train_df.labels.unique().tolist()","metadata":{"execution":{"iopub.status.busy":"2022-01-03T20:17:25.835288Z","iopub.execute_input":"2022-01-03T20:17:25.836049Z","iopub.status.idle":"2022-01-03T20:17:25.840407Z","shell.execute_reply.started":"2022-01-03T20:17:25.836010Z","shell.execute_reply":"2022-01-03T20:17:25.839706Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Drop the instances with no text\ntrain_df = train_df.dropna()\ntest_df = test_df.dropna()\nprint(\"Train shape: {}, Test shape: {}.\".format(train_df.shape, test_df.shape))","metadata":{"execution":{"iopub.status.busy":"2022-01-03T20:17:25.841589Z","iopub.execute_input":"2022-01-03T20:17:25.842353Z","iopub.status.idle":"2022-01-03T20:17:25.854186Z","shell.execute_reply.started":"2022-01-03T20:17:25.842293Z","shell.execute_reply":"2022-01-03T20:17:25.853233Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Transformers","metadata":{}},{"cell_type":"markdown","source":"Let's start with arguments which are the same for all the models.","metadata":{}},{"cell_type":"code","source":"# define hyperparameters\nmodel_args ={\"overwrite_output_dir\": True,\n             \"num_train_epochs\": 90,\n             \"labels_list\": LABELS,\n             \"learning_rate\": 1e-5,\n             \"train_batch_size\": 32,\n             \"no_cache\": True,\n             \"no_save\": True,\n             \"max_seq_length\": 300,\n             \"save_steps\": -1,\n             }","metadata":{"execution":{"iopub.status.busy":"2022-01-03T20:17:25.855595Z","iopub.execute_input":"2022-01-03T20:17:25.856154Z","iopub.status.idle":"2022-01-03T20:17:25.861273Z","shell.execute_reply.started":"2022-01-03T20:17:25.856117Z","shell.execute_reply":"2022-01-03T20:17:25.860553Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"import gc\nimport torch\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2022-01-03T20:17:25.862723Z","iopub.execute_input":"2022-01-03T20:17:25.863404Z","iopub.status.idle":"2022-01-03T20:17:26.276260Z","shell.execute_reply.started":"2022-01-03T20:17:25.863356Z","shell.execute_reply":"2022-01-03T20:17:26.275528Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### SloBERTa\nSlovene model\nhttps://huggingface.co/EMBEDDIA/sloberta","metadata":{}},{"cell_type":"markdown","source":"from simpletransformers.classification import ClassificationModel\n\nsloberta_model = ClassificationModel(\n    \"camembert\", \"EMBEDDIA/sloberta\",\n    use_cuda = True,\n    num_labels = 21,\n    args = model_args)","metadata":{"execution":{"iopub.status.busy":"2022-01-03T16:29:18.518235Z","iopub.execute_input":"2022-01-03T16:29:18.518561Z","iopub.status.idle":"2022-01-03T16:29:23.999386Z","shell.execute_reply.started":"2022-01-03T16:29:18.518522Z","shell.execute_reply":"2022-01-03T16:29:23.998564Z"}}},{"cell_type":"markdown","source":"### CroSloEngual BERT\nSlovene-Croatian-English model\nhttps://huggingface.co/EMBEDDIA/crosloengual-bert","metadata":{}},{"cell_type":"markdown","source":"from simpletransformers.classification import ClassificationModel\n\ncrosloengualbert_model = ClassificationModel(\n        \"bert\", \"EMBEDDIA/crosloengual-bert\",\n        num_labels=21,\n        use_cuda=True,\n        args=model_args\n    )","metadata":{"execution":{"iopub.status.busy":"2022-01-03T18:56:58.240818Z","iopub.execute_input":"2022-01-03T18:56:58.241098Z","iopub.status.idle":"2022-01-03T18:57:04.424455Z","shell.execute_reply.started":"2022-01-03T18:56:58.241066Z","shell.execute_reply":"2022-01-03T18:57:04.423670Z"}}},{"cell_type":"markdown","source":"### Base-sized XLM-RoBERTa\n\nMultilingual model\nhttps://huggingface.co/xlm-roberta-base","metadata":{}},{"cell_type":"markdown","source":"from simpletransformers.classification import ClassificationModel\n\nroberta_base_model = ClassificationModel(\n        \"xlmroberta\", \"xlm-roberta-base\",\n        num_labels=21,\n        use_cuda=True,\n        args=model_args\n    )","metadata":{"execution":{"iopub.status.busy":"2022-01-03T19:10:57.479779Z","iopub.execute_input":"2022-01-03T19:10:57.480105Z","iopub.status.idle":"2022-01-03T19:11:58.274814Z","shell.execute_reply.started":"2022-01-03T19:10:57.480071Z","shell.execute_reply":"2022-01-03T19:11:58.273784Z"}}},{"cell_type":"markdown","source":"### Large-sized XML-RoBERTa\nMultilingual model https://huggingface.co/xlm-roberta-large","metadata":{}},{"cell_type":"markdown","source":"from simpletransformers.classification import ClassificationModel\n\nroberta_large_model = ClassificationModel(\n        \"xlmroberta\", \"xlm-roberta-large\",\n        num_labels=21,\n        use_cuda=True,\n        args=model_args\n    )","metadata":{"execution":{"iopub.status.busy":"2022-01-03T20:07:36.134730Z","iopub.execute_input":"2022-01-03T20:07:36.136909Z","iopub.status.idle":"2022-01-03T20:07:50.811525Z","shell.execute_reply.started":"2022-01-03T20:07:36.136868Z","shell.execute_reply":"2022-01-03T20:07:50.809439Z"}}},{"cell_type":"markdown","source":"### DeBERTaV3\nMultilingual model https://huggingface.co/microsoft/mdeberta-v3-base","metadata":{}},{"cell_type":"markdown","source":"from simpletransformers.classification import ClassificationModel\n\ndebertav3_model = ClassificationModel(\n        \"debertav2\", \"microsoft/mdeberta-v3-base\",\n        num_labels=21,\n        use_cuda=True,\n        args=model_args\n    )","metadata":{"execution":{"iopub.status.busy":"2022-01-03T19:25:30.459204Z","iopub.execute_input":"2022-01-03T19:25:30.462074Z","iopub.status.idle":"2022-01-03T19:25:42.477123Z","shell.execute_reply.started":"2022-01-03T19:25:30.462035Z","shell.execute_reply":"2022-01-03T19:25:42.476282Z"}}},{"cell_type":"markdown","source":"### BERTić\nModel for related South Slavic languages https://huggingface.co/classla/bcms-bertic","metadata":{}},{"cell_type":"markdown","source":"from simpletransformers.classification import ClassificationModel\n\nbertic_model = ClassificationModel(\n        \"electra\", \"classla/bcms-bertic\",\n        num_labels=21,\n        use_cuda=True,\n        args=model_args\n    )","metadata":{"execution":{"iopub.status.busy":"2022-01-03T20:12:05.456734Z","iopub.execute_input":"2022-01-03T20:12:05.457033Z","iopub.status.idle":"2022-01-03T20:12:32.682960Z","shell.execute_reply.started":"2022-01-03T20:12:05.456990Z","shell.execute_reply":"2022-01-03T20:12:32.682204Z"}}},{"cell_type":"markdown","source":"### BERT base model (cased)\nMonolingual English model https://huggingface.co/bert-base-cased","metadata":{}},{"cell_type":"code","source":"from simpletransformers.classification import ClassificationModel\n\nbertbase_model = ClassificationModel(\n        \"bert\", \"bert-base-cased\",\n        num_labels=21,\n        use_cuda=True,\n        args=model_args\n    )","metadata":{"execution":{"iopub.status.busy":"2022-01-03T20:17:26.277703Z","iopub.execute_input":"2022-01-03T20:17:26.277950Z","iopub.status.idle":"2022-01-03T20:17:34.462616Z","shell.execute_reply.started":"2022-01-03T20:17:26.277915Z","shell.execute_reply":"2022-01-03T20:17:34.461738Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Training and evaluation","metadata":{}},{"cell_type":"markdown","source":"### Train","metadata":{}},{"cell_type":"code","source":"# SloBERTa\n#sloberta_model.train_model(train_df)\n\n# CroSloEngual BERT\n#crosloengualbert_model.train_model(train_df)\n\n# Base-sized XML-Roberta\n#roberta_base_model.train_model(train_df)\n\n# Large-sized XML-Roberta\n#roberta_large_model.train_model(train_df)\n\n# DeBERTav3\n#debertav3_model.train_model(train_df)\n\n# BERTić\n#bertic_model.train_model(train_df)\n\n# English base-sized BERT\nbertbase_model.train_model(train_df)","metadata":{"execution":{"iopub.status.busy":"2022-01-03T20:17:34.467364Z","iopub.execute_input":"2022-01-03T20:17:34.469851Z","iopub.status.idle":"2022-01-03T20:17:46.971514Z","shell.execute_reply.started":"2022-01-03T20:17:34.469771Z","shell.execute_reply":"2022-01-03T20:17:46.969485Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### Evaluate","metadata":{}},{"cell_type":"code","source":"def eval_model(model,plot_title=None):\n    \"\"\" Evaluates the model. It takes the test data, named as \"test_df\", and labels list named \"LABELS\".\n    \n    Args: \n        model (simpletransformers.ClassificationModel): the model name.\n        plot_title (string): the title of the confusion matrix, defaults to None.\n    \n    Returns:\n    \n    \"\"\"\n    instance_predictions, raw_outputs = sloberta_model.predict(['Danes poročamo o dogodku, ki se je zgodil 1. 1. 2020. Oseba je dejala:\"To je res nenormalen dogodek\"'])\n    print(\"Instance prediction: \", instance_predictions)\n    \n    # Get the true labels from the dataframe\n    y_true = test_df.labels\n\n    # Calculate the model's predictions\n    y_pred = model.predict(test_df.text.tolist())[0]\n    \n    macro = f1_score(y_true, y_pred, labels=LABELS, average=\"macro\")\n    micro = f1_score(y_true, y_pred, labels=LABELS,  average=\"micro\")\n    print(f\"Macro f1: {macro:0.3}\\nMicro f1: {micro:0.3}\")\n    \n    cm = confusion_matrix(y_true, y_pred, labels=LABELS)\n    plt.figure(figsize=(9, 9))\n    plt.imshow(cm, cmap=\"Oranges\")\n    for (i, j), z in np.ndenumerate(cm):\n        plt.text(j, i, '{:d}'.format(z), ha='center', va='center')\n    classNames = labels\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    tick_marks = np.arange(len(classNames))\n    plt.xticks(tick_marks, classNames, rotation=90)\n    plt.yticks(tick_marks, classNames)\n\n    metrics = f\"{microF1:0.4}, {macroF1:0.4}\"\n    if title:\n        plt.title(title +\";\\n\" + metrics)\n    else:\n        plt.title(metrics)\n    plt.tight_layout()\n    plt.show()\n    return microF1, macroF1","metadata":{"execution":{"iopub.status.busy":"2022-01-03T20:17:46.972764Z","iopub.status.idle":"2022-01-03T20:17:46.973322Z","shell.execute_reply.started":"2022-01-03T20:17:46.973059Z","shell.execute_reply":"2022-01-03T20:17:46.973086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Choose from the following models:\nsloberta_model(train_df), crosloengualbert_model(train_df), roberta_base_model(train_df), roberta_large_model(train_df), debertav3_model(train_df), bertic_model(train_df), bertbase_model(train_df)","metadata":{}},{"cell_type":"code","source":"eval_model(crosloengualbert,title=\"CroSloEngualBert-300-Initial_Setup\")\n#plt.savefig(\"SloBERTa-Initial-Setup_test_dev.png\")","metadata":{"execution":{"iopub.status.busy":"2022-01-03T20:17:46.975209Z","iopub.status.idle":"2022-01-03T20:17:46.975669Z","shell.execute_reply.started":"2022-01-03T20:17:46.975433Z","shell.execute_reply":"2022-01-03T20:17:46.975456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Produce the confusion matrix:","metadata":{}},{"cell_type":"code","source":"def plot_cm(y_true, y_pred, labels, title=None):\n    cm = confusion_matrix(y_true, y_pred, labels=labels, )\n    # print(cm)\n    plt.figure(figsize=(9, 9))\n    plt.imshow(cm, cmap=\"Oranges\")\n    for (i, j), z in np.ndenumerate(cm):\n        plt.text(j, i, '{:d}'.format(z), ha='center', va='center')\n    classNames = labels\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    tick_marks = np.arange(len(classNames))\n    plt.xticks(tick_marks, classNames, rotation=90)\n    plt.yticks(tick_marks, classNames)\n    microF1 = f1_score(y_true, y_pred, labels=labels, average =\"micro\")\n    macroF1 = f1_score(y_true, y_pred, labels=labels, average =\"macro\")\n\n    print(f\"Micro F1: {microF1:0.3}\",f\"Macro F1: {macroF1:0.3}\")\n\n    metrics = f\"{microF1:0.4}, {macroF1:0.4}\"\n    if title:\n        plt.title(title +\";\\n\" + metrics)\n    else:\n        plt.title(metrics)\n    plt.tight_layout()\n    plt.show()\n    return microF1, macroF1","metadata":{"execution":{"iopub.status.busy":"2022-01-03T20:17:46.977160Z","iopub.status.idle":"2022-01-03T20:17:46.977616Z","shell.execute_reply.started":"2022-01-03T20:17:46.977373Z","shell.execute_reply":"2022-01-03T20:17:46.977398Z"},"trusted":true},"execution_count":null,"outputs":[]}]}