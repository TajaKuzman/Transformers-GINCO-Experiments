{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we include the code for only_primary, dedup text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code from utils.py. This code includes downsampling and experiments with secondary label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import parse\n",
    "import fasttext\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "train_labels = ['__label__Legal/Regulation', '__label__Opinionated_News', '__label__News/Reporting', '__label__Forum', '__label__Correspondence', '__label__Invitation', '__label__Instruction', '__label__Recipe', '__label__Opinion/Argumentation', '__label__Promotion_of_Services',\n",
    "                '__label__Promotion', '__label__List_of_Summaries/Excerpts', '__label__Promotion_of_a_Product', '__label__Call', '__label__Review', '__label__Other', '__label__Information/Explanation', '__label__Interview', '__label__Prose', '__label__Research_Article', '__label__Announcement']\n",
    "STR_TO_NUM = {s: i for i, s in enumerate(train_labels)}\n",
    "NUM_TO_STR = {i: s for i, s in enumerate(train_labels)}\n",
    "NUM_TO_STR_NO_PREFIX = {i: s[9:].replace(\n",
    "    \"_\", \" \") for i, s in enumerate(train_labels)}\n",
    "train_labels_no_prefix = [s[9:].replace(\"_\", \" \") for s in train_labels]\n",
    "\n",
    "reduced_labels = ['__label__Legal/Regulation', '__label__Opinionated_News', '__label__News/Reporting', '__label__Forum', '__label__Instruction', '__label__Opinion/Argumentation',\n",
    "                  '__label__Promotion', '__label__List_of_Summaries/Excerpts', '__label__Other', '__label__Information/Explanation', '__label__Interview', '__label__Announcement']\n",
    "REDUCED_STR_TO_NUM = {s: i for i, s in enumerate(reduced_labels)}\n",
    "REDUCED_NUM_TO_STR = {i: s for i, s in enumerate(reduced_labels)}\n",
    "REDUCED_NUM_TO_STR_NO_PREFIX = {i: s[9:].replace(\n",
    "    \"_\", \" \") for i, s in enumerate(reduced_labels)}\n",
    "reduced_labels_no_prefix = [s[9:].replace(\"_\", \" \") for s in reduced_labels]\n",
    "\n",
    "\n",
    "list_of_categories_matrix = ['Information/Explanation', 'Research Article', 'Instruction', 'Recipe', 'Legal/Regulation', 'Call', 'Announcement', 'News/Reporting', 'Opinionated News',\n",
    "                             'Opinion/Argumentation', 'Review', 'Promotion', 'Promotion of a Product', 'Promotion of Services', 'Invitation', 'Forum', 'Interview', 'Correspondence', 'Prose', 'List of Summaries/Excerpts', 'Other']\n",
    "list_of_categories_matrix_donwnsampled = ['Information/Explanation', 'Instruction', 'Legal/Regulation', 'Announcement', 'News/Reporting',\n",
    "                                          'Opinionated News', 'Opinion/Argumentation', 'Promotion', 'Forum', 'Interview', 'List of Summaries/Excerpts', 'Other']\n",
    "\n",
    "\n",
    "def parse_fasttext_file(path: str, encode=True):\n",
    "    \"\"\"Reads fasttext formatted file and returns dataframe.\n",
    "\n",
    "    Args:\n",
    "        path (str): fasttext file to be parsed.\n",
    "        encode (bool, optional): Whether the labels should be encoded to integers. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DF with columns `text` and `labels`.\n",
    "    \"\"\"\n",
    "    with open(path, \"r\") as f:\n",
    "        content = f.readlines()\n",
    "    pattern = \"{label} {text}\\n\"\n",
    "    p = parse.compile(pattern)\n",
    "\n",
    "    labels, texts = list(), list()\n",
    "    for line in content:\n",
    "        rez = p.parse(line)\n",
    "        if rez is not None:\n",
    "            if rez[\"label\"] == '__label__Promotion_of_services':\n",
    "                labels.append('__label__Promotion_of_Services')\n",
    "            elif rez[\"label\"] == '__label__Promotion_of_a_product':\n",
    "                labels.append('__label__Promotion_of_a_Product')\n",
    "            else:\n",
    "                labels.append(rez[\"label\"])\n",
    "            texts.append(rez[\"text\"])\n",
    "        else:\n",
    "            pass\n",
    "    if encode:\n",
    "        labels = [STR_TO_NUM[i] for i in labels]\n",
    "    return pd.DataFrame(data={\"text\": texts, \"labels\": labels})\n",
    "\n",
    "\n",
    "def train_model(train_df, NUM_EPOCHS=30, num_labels=21, use_cuda=True, no_cache=True,\n",
    "                labels=None):\n",
    "    \"\"\"Trains a simpletransformer model and returns it.\n",
    "\n",
    "    Args:\n",
    "        train_df (pandas.DataFrame): A DataFrame with columns [\"text\", \"labels\"].\n",
    "        NUM_EPOCHS (int, optional): Number of epochs. Defaults to 30.\n",
    "        num_labels (int, optional): Number of labels used. Defaults to 21.\n",
    "        use_cuda (bool, optional): Whether to use cuda. Defaults to True. Set False for easier debugging.\n",
    "        no_cache (bool, optional): Whether to use caching or not. Defaults to True.\n",
    "        labels (list(str), optional): If not None, use these labels to use string labels instead of numeric labels. \n",
    "            Defaults to None. If set, num_labels is calculated automatically.\n",
    "\n",
    "    Returns:\n",
    "        simpletransformers.ClassificationModel: a trained model\n",
    "    \"\"\"\n",
    "    from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "\n",
    "    model_args = ClassificationArgs()\n",
    "    model_args.num_train_epochs = NUM_EPOCHS\n",
    "    model_args.learning_rate = 1e-5\n",
    "    model_args.overwrite_output_dir = True\n",
    "    model_args.train_batch_size = 32\n",
    "    model_args.no_cache = no_cache\n",
    "    model_args.no_save = True\n",
    "    model_args.save_steps = -1\n",
    "    model_args.max_seq_length = 512\n",
    "    model_args.silent = True\n",
    "    if labels:\n",
    "        LABELS = list(LABELS)\n",
    "        model_args.labels_list = LABELS\n",
    "        num_labels = len(LABELS)\n",
    "\n",
    "    model = ClassificationModel(\n",
    "        \"camembert\", \"EMBEDDIA/sloberta\",\n",
    "        num_labels=num_labels,\n",
    "        use_cuda=use_cuda,\n",
    "        args=model_args\n",
    "    )\n",
    "    model.train_model(train_df)\n",
    "    return model\n",
    "\n",
    "\n",
    "def eval_model(test_df, model):\n",
    "    \"\"\"Evaluates trained model on test_df and returns metrics.\n",
    "\n",
    "    Args:\n",
    "        test_df (pd.DataFrame): dataframe with `text` and `labels` columns\n",
    "        model (simpletransformers.ClassificationModel): previously trained model to evaluate.\n",
    "\n",
    "    Returns:\n",
    "        results (dict): dictionary with fields `microF1`, `macroF1`, `y_true`, `y_pred`.\n",
    "    \"\"\"\n",
    "    y_true_enc = test_df.labels\n",
    "    y_pred_enc = model.predict(test_df.text.tolist())[0]\n",
    "\n",
    "    y_true = [NUM_TO_STR[i] for i in y_true_enc]\n",
    "    y_pred = [NUM_TO_STR[i] for i in y_pred_enc]\n",
    "\n",
    "    microF1 = f1_score(y_true, y_pred, labels=train_labels, average=\"micro\")\n",
    "    macroF1 = f1_score(y_true, y_pred, labels=train_labels, average=\"macro\")\n",
    "\n",
    "    return {\"microF1\": microF1,\n",
    "            \"macroF1\": macroF1,\n",
    "            \"y_true\": y_true_enc.tolist(),\n",
    "            \"y_pred\": y_pred_enc.tolist()}\n",
    "\n",
    "\n",
    "def plot_cm(y_true, y_pred,  save=False, title=None, labels=None,\n",
    "            include_metrics=True, figsize=None):\n",
    "    \"\"\"Plots confusion matrix for y_true and y_pred. Can calculate\n",
    "    metrics and display them in the title.\n",
    "\n",
    "    Args:\n",
    "        y_true (list|np.array|pd.Series): true labels\n",
    "        y_pred (list|np.array|pd.Series): predicted labels\n",
    "        save (bool|str, optional): string, path to save a figure to, or bool (False) to not save. Defaults to False.\n",
    "        title ([None|str], optional): Title to put at the top of the figure. Defaults to None.\n",
    "        labels (list|np.array|pd.Series, optional): how to arrange labels. The list must contain all labels\n",
    "            that appear in y_true and y_pred. Defaults to None.\n",
    "        include_metrics (bool, optional): Whether the metrics should be displayed\n",
    "            under the title. Defaults to True.\n",
    "        figsize (tuple(int, int), optional): figure size. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        microF1, macroF1: metrics.\n",
    "    \"\"\"            \n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn.metrics import f1_score\n",
    "    import matplotlib.pyplot as plt\n",
    "    if not labels:\n",
    "        labels = list_of_categories_matrix\n",
    "    plt.style.use([\"science\", \"no-latex\", ])\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels, )\n",
    "    cm = cm/3\n",
    "    cm = cm.astype(int)\n",
    "    # print(cm)\n",
    "    if figsize == None:\n",
    "        figsize = (9, 9)\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(cm, cmap=\"Oranges\")\n",
    "    for (i, j), z in np.ndenumerate(cm):\n",
    "        plt.text(j, i, '{:d}'.format(z), ha='center', va='center')\n",
    "    classNames = labels\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    tick_marks = np.arange(len(classNames))\n",
    "    plt.xticks(tick_marks, classNames, rotation=90)\n",
    "    plt.yticks(tick_marks, classNames)\n",
    "    microF1 = f1_score(y_true, y_pred, labels=labels, average=\"micro\")\n",
    "    macroF1 = f1_score(y_true, y_pred, labels=labels, average=\"macro\")\n",
    "\n",
    "    print(f\"{microF1=:0.4}\")\n",
    "    print(f\"{macroF1=:0.4}\")\n",
    "\n",
    "    metrics = f\"{microF1=:0.4}, {macroF1=:0.4}\" if include_metrics else \"\"\n",
    "    if title:\n",
    "        if include_metrics:\n",
    "            plt.title(title + \";\\n\" + metrics)\n",
    "        else:\n",
    "            plt.title(title)\n",
    "    else:\n",
    "        plt.title(metrics)\n",
    "    plt.tight_layout()\n",
    "    if save:\n",
    "        plt.savefig(save)\n",
    "    plt.show()\n",
    "    return microF1, macroF1\n",
    "\n",
    "\n",
    "def read_record(filename: str) -> pd.DataFrame:\n",
    "    \"\"\"Reads a record file and returns a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        filename (str): record filename\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: resulting dataframe\n",
    "    \"\"\"\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    pd.set_option(\"precision\", 3)\n",
    "    with open(filename) as f:\n",
    "        content = json.load(f)\n",
    "    jsonlikecontent = dict()\n",
    "    for key in content[0].keys():\n",
    "        jsonlikecontent[key] = [i[key] for i in content]\n",
    "    df = pd.DataFrame(data=jsonlikecontent)\n",
    "    return df\n",
    "\n",
    "\n",
    "def downsample_second(numlabel: int) -> int:\n",
    "    \"\"\"Performs the downsampling based on the second downsampling.\n",
    "\n",
    "    Args:\n",
    "        numlabel (int): numerical label, original\n",
    "\n",
    "    Returns:\n",
    "        int: downsampled numerical label\n",
    "    \"\"\"\n",
    "    stringlabel = NUM_TO_STR[numlabel]\n",
    "    second_original = {\"Recipe\": \"Instruction\", \"Research Article\": \"Information/Explanation\", \"Review\": \"Opinion/Argumentation\", \"Promotion of Services\": \"Promotion\",\n",
    "                       \"Promotion of a Product\": \"Promotion\", \"Invitation\": \"Promotion\", \"Correspondence\": \"Other\", \"Prose\": \"Other\", \"Call\": \"Other\"}\n",
    "\n",
    "    second = {\n",
    "        f\"__label__{k.replace(' ', '_')}\": f\"__label__{v.replace(' ', '_')}\" for k, v in second_original.items()}\n",
    "\n",
    "    new_stringlabel = second.get(stringlabel, stringlabel)\n",
    "\n",
    "    return REDUCED_STR_TO_NUM[new_stringlabel]\n",
    "\n",
    "\n",
    "def to_label(l: list, reduced=False) -> list:\n",
    "    \"\"\"Transform a series of strings or integers into original labels.\n",
    "\n",
    "    Args:\n",
    "        l (list): list or pandas.Series with either string or numeric labels.\n",
    "        reduced (bool, optional): if True, use reduced label set (12 labels). Defaults to False.\n",
    "\n",
    "    Raises:\n",
    "        AttributeError: there seems to be a weird input type\n",
    "\n",
    "    Returns:\n",
    "        list: list of labels with no prefix\n",
    "    \"\"\"    \n",
    "    \n",
    "    to_return = list()\n",
    "    for i in l:\n",
    "        if type(i) == int:\n",
    "            if reduced:\n",
    "                to_return.append(REDUCED_NUM_TO_STR_NO_PREFIX[i])\n",
    "            else:\n",
    "                to_return.append(NUM_TO_STR_NO_PREFIX[i])\n",
    "        elif type(i) == str:\n",
    "            to_return.append(i[9:].replace(\"_\", \" \"))\n",
    "        else:\n",
    "            raise AttributeError(f\"Got type {type(i)} for input {i}\")\n",
    "    return to_return\n",
    "\n",
    "\n",
    "def train_model_xlm_roberta(train_df, NUM_EPOCHS=30, num_labels=21, use_cuda=True, no_cache=True):\n",
    "    from simpletransformers.classification import ClassificationModel\n",
    "    model_args = {\n",
    "        \"num_train_epochs\": NUM_EPOCHS,\n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"overwrite_output_dir\": True,\n",
    "        \"train_batch_size\": 32,\n",
    "        \"no_save\": True,\n",
    "        \"no_cache\": no_cache,\n",
    "        \"overwrite_output_dir\": True,\n",
    "        \"save_steps\": -1,\n",
    "        \"max_seq_length\": 512,\n",
    "        \"silent\": True\n",
    "    }\n",
    "\n",
    "    model = ClassificationModel(\n",
    "        \"xlm-roberta\", \"xlm-roberta-base\",\n",
    "        num_labels=num_labels,\n",
    "        use_cuda=use_cuda,\n",
    "        args=model_args\n",
    "    )\n",
    "    model.train_model(train_df)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dd = \"dev_onlyprimary_True_dedup_True_only_keep_False.fasttext\"\n",
    "test_dd = \"test_onlyprimary_True_dedup_True_only_keep_False.fasttext\"\n",
    "train_dd = \"train_onlyprimary_False_dedup_True_only_keep_False.fasttext\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in [ train_dd,  test_dd,  dev_dd]:\n",
    "    try:\n",
    "        _ = parse_fasttext_file(filename)\n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dd_df = parse_fasttext_file(dev_dd)\n",
    "test_dd_df = parse_fasttext_file(test_dd)\n",
    "train_dd_df = parse_fasttext_file(train_dd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create multiple runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(5):\n",
    "    print(\"Run \", i+1, \"of 5\")\n",
    "    model = train_model(train_dd_df)\n",
    "\n",
    "    rundict = eval_model(test_dd_df, model)\n",
    "    rundict[\"train\"] = \"secondary_dd\"\n",
    "    rundict[\"eval\"] = \"test_dd\"\n",
    "    results.append(rundict)\n",
    "\n",
    "    rundict = eval_model(dev_dd_df, model)\n",
    "    rundict[\"train\"] = \"secondary_dd\"\n",
    "    rundict[\"eval\"] = \"dev_dd\"\n",
    "    results.append(rundict)\n",
    "\n",
    "    devtest_dd_df = pd.concat([test_dd_df, dev_dd_df], ignore_index=True)\n",
    "\n",
    "    rundict = eval_model(devtest_dd_df, model)\n",
    "    rundict[\"train\"] = \"secondary_dd\"\n",
    "    rundict[\"eval\"] = \"devtest_dd\"\n",
    "    results.append(rundict)\n",
    "\n",
    "with open(\"backup_18_2.txt\", \"w\") as f:\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's another code, for training with dev added to train.\n",
    "\n",
    "Here, the files from fasttext2 are used:\n",
    "datadir = \"/home/peterr/macocu/task5_webgenres/data/final/fasttext2\"\n",
    "\n",
    "dev_dd = os.path.join(datadir, \"dev_onlyprimary_True_dedup_True.fasttext\")\n",
    "test_dd = os.path.join(datadir, \"test_onlyprimary_True_dedup_True.fasttext\")\n",
    "train_dd = os.path.join(datadir, \"train_onlyprimary_True_dedup_True.fasttext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(\"run: \", i)\n",
    "\n",
    "    traindev = pd.concat([dev_dd_df, train_dd_df], ignore_index=True)\n",
    "\n",
    "    model = train_model(traindev)\n",
    "\n",
    "    rundict = eval_model(test_dd_df, model)\n",
    "    rundict[\"train\"] = \"traindev_dd\"\n",
    "    rundict[\"eval\"] = \"test_dd\"\n",
    "\n",
    "    results.append(rundict)\n",
    "\n",
    "import json\n",
    "with open(\"backup_20.txt\", \"w\") as f:\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To analyse the results (create a table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_record\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.concat(\n",
    "    [read_record(file) for file in [\n",
    "    \"backup_17.txt\",\n",
    "    \"backup_17__2.txt\",\n",
    "    \"backup_18.txt\",\n",
    "    \"backup_18_2.txt\",\n",
    "    \"backup_20.txt\",\n",
    "    \"backup_20_2.txt\"\n",
    "]\n",
    "\n",
    "], ignore_index=True)\n",
    "\n",
    "c_dd = df[\"eval\"] == \"test_dd\"\n",
    "c_train_dd = df.train == \"dd\"\n",
    "c_traindev_dd = df.train == \"traindev_dd\"\n",
    "\n",
    "\n",
    "\n",
    "only_train = c_dd & c_train_dd\n",
    "train_dev = c_traindev_dd & c_dd\n",
    "\n",
    "df[\"scenario\"] = \"\"\n",
    "\n",
    "df.loc[only_train, \"scenario\"] = \"train only\"\n",
    "df.loc[train_dev, \"scenario\"] = \"train + dev\"\n",
    "\n",
    "print(df[df.scenario!=\"\"].groupby(\"scenario\").agg(\"mean,std\".split(\",\")).to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate statistical significance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [\"microF1\", \"macroF1\"]:\n",
    "\n",
    "    from scipy import stats\n",
    "    #Candidate:\n",
    "    higher = df.loc[df.scenario == \"train + dev\", col].values\n",
    "    #Alternative\n",
    "    lower = df.loc[df.scenario == \"train only\", col].values\n",
    "\n",
    "    # print(f\"Wilcoxon p value: {stats.wilcoxon(higher,lower, alternative='greater')[1]:0.3}\", \"\\t\\t(alternative hypothesis: first is greater than the second)\")\n",
    "    print(\"Evaluating \", col, \", presuming more data is better.\")\n",
    "    print(f\"MannWhithey p value: {stats.mannwhitneyu(higher,lower, alternative='greater')[1]:0.3}\", \"\\t\\t(alternative hypothesis: first is greater than the second)\")\n",
    "\n",
    "    print(f\"Student p value: {stats.ttest_ind(higher,lower)[1]:0.3}\", \"\\t\\t(null hypothesis: samples have identical average, equal variance is assumed but not necessary)\")\n",
    "    import numpy as np\n",
    "    print(f\"Higher average: {np.mean(higher):0.4}, lower average: {np.mean(lower):0.4}\")\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "c1 = df.train == \"dd\"\n",
    "c2 = df.train == \"minitrain_dd\"\n",
    "c3 = df.train == \"traindev_dd\"\n",
    "c4 = df[\"eval\"] == \"test_dd\"\n",
    "\n",
    "\n",
    "\n",
    "gb = df[(c1|c2|c3)&c4].groupby(\"train\")[[\"microF1\", \"macroF1\"]].agg([\"mean\", \"std\"])\n",
    "gb = gb.loc[[\"minitrain_dd\", \"dd\", \"traindev_dd\"], :]\n",
    "gb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context([\"science\", \"no-latex\"]):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.errorbar(train_fraction, mimean, yerr=mistd, label=\"micro F1\")\n",
    "    plt.errorbar(train_fraction, mamean, yerr=mastd, label=\"macro F1\", color=\"tab:orange\")\n",
    "\n",
    "    plt.xlabel(\"Fraction of data to train on\")\n",
    "    plt.ylabel(\"Metric value\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(\"images/20_effect_of_training_split_size.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = df.scenario == \"train + dev\"\n",
    "\n",
    "y_true = np.array([i for i in df[c].y_true.values]).reshape(-1)\n",
    "y_pred = np.array([i for i in df[c].y_pred.values]).reshape(-1)\n",
    "\n",
    "from utils import NUM_TO_STR\n",
    "y_true = [NUM_TO_STR[i] for i in y_true]\n",
    "y_pred = [NUM_TO_STR[i] for i in y_pred]\n",
    "\n",
    "from utils import plot_cm, train_labels\n",
    "\n",
    "plot_cm(y_true, y_pred, save=\"images/20_traindev_all_cm.png\",\n",
    "title=\"Cumulative CM for train dd + dev dd, test dd\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
